{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This project aims to train a machine learning classifier that reports the attitude of a message (or post) towards vaccine-related topics.\n",
    "\n",
    "# Methods\n",
    "__The data__ used in this project comes from the author of the article '[The Small, Small World of Facebook’s Anti-vaxxers](https://www.theatlantic.com/health/archive/2019/02/anti-vaxx-facebook-social-media/583681/)', Alexis C. Madrigal. By using the web-monitoring tool CrowdTangle, she analyzed the most popular posts since 2016 that contain the word _vaccine_ from Facebook. [Her data](https://docs.google.com/spreadsheets/d/1j6tJDlMJErjBwoxLh4GpzCV4nM00nQS_1faZMXPGcxw/edit#gid=1593598275) is publicly available through a link included in the article.\n",
    "<br>\n",
    "\n",
    "__The steps__ included in this projects are as follow,\n",
    "1. Identify the pro-vaccine and anti-vaccine posts to be used to train the model and extract these posts from the dataset\n",
    "2. Label (i.e., provaccine/anti-vaccine) each post\n",
    "3. Process the message of the post to an analyzable format using Natural Language Processing techniques\n",
    "4. Prepare the data for training a machine learning model\n",
    "5. Train several models\n",
    "\n",
    "# Reading Guide\n",
    "I try to document everything from lay people's perspective. You shouldn't need a programming background to understand my note and interpretation in this notebook. However, there will be some Python related notes for replication purposes. Those notes are left directly with the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the packages used in this project. Packages used include nltk, numpy, pandas, pickle, random, re, sklearn, statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import pickle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to a pandas dataframe for data manipulation. Display the first 5 rows for an initial check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page Name</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Page Id</th>\n",
       "      <th>Page Likes at Posting</th>\n",
       "      <th>Created</th>\n",
       "      <th>Type</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Love</th>\n",
       "      <th>...</th>\n",
       "      <th>URL</th>\n",
       "      <th>Message</th>\n",
       "      <th>Link</th>\n",
       "      <th>Final Link</th>\n",
       "      <th>Link Text</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sponsor Id</th>\n",
       "      <th>Sponsor Name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planet Paws</td>\n",
       "      <td>PlanetPaws.ca</td>\n",
       "      <td>1.124380e+14</td>\n",
       "      <td>1578676</td>\n",
       "      <td>2017-02-26 09:50:24 EST</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>75461</td>\n",
       "      <td>42879</td>\n",
       "      <td>894482</td>\n",
       "      <td>2406</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/PlanetPaws.ca/posts/1...</td>\n",
       "      <td>Over-vaccinating and the overdosing of pet vac...</td>\n",
       "      <td>https://www.facebook.com/PlanetPaws.ca/videos/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Dangers of Vaccine Overdosing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1091367.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalie Bomke Fox 32 Chicago</td>\n",
       "      <td>NatalieBomkeFox32Chicago</td>\n",
       "      <td>1.668040e+14</td>\n",
       "      <td>112316</td>\n",
       "      <td>2018-02-02 09:30:00 EST</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>135485</td>\n",
       "      <td>41642</td>\n",
       "      <td>627972</td>\n",
       "      <td>27306</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/NatalieBomkeFox32Chic...</td>\n",
       "      <td>CANCER VACCINE SUCCESSFUL IN TESTING... Stanfo...</td>\n",
       "      <td>https://www.facebook.com/NatalieBomkeFox32Chic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CANCER VACCINE SUCCESSFUL IN TESTING...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>861565.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOX 2 Detroit</td>\n",
       "      <td>WJBKFox2Detroit</td>\n",
       "      <td>3.636580e+11</td>\n",
       "      <td>819062</td>\n",
       "      <td>2018-02-05 14:45:02 EST</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>84593</td>\n",
       "      <td>15447</td>\n",
       "      <td>580983</td>\n",
       "      <td>14496</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/WJBKFox2Detroit/posts...</td>\n",
       "      <td>CANCER VACCINE SUCCESSFUL IN TESTING: The vacc...</td>\n",
       "      <td>https://www.facebook.com/WJBKFox2Detroit/video...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CANCER VACCINE SUCCESSFUL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712774.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gizmodo</td>\n",
       "      <td>gizmodo</td>\n",
       "      <td>5.718759e+09</td>\n",
       "      <td>1555197</td>\n",
       "      <td>2017-11-20 17:40:46 EST</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>122206</td>\n",
       "      <td>20150</td>\n",
       "      <td>433945</td>\n",
       "      <td>11158</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/gizmodo/posts/1015594...</td>\n",
       "      <td>Paul Alexander spends nearly every hour, of ev...</td>\n",
       "      <td>https://www.facebook.com/gizmodo/videos/101559...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Last of the Iron Lungs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>674298.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hashem Al-Ghaili</td>\n",
       "      <td>ScienceNaturePage</td>\n",
       "      <td>6.935050e+14</td>\n",
       "      <td>13426659</td>\n",
       "      <td>2018-06-22 11:08:29 EDT</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>40464</td>\n",
       "      <td>3112</td>\n",
       "      <td>322544</td>\n",
       "      <td>3762</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/ScienceNaturePage/pos...</td>\n",
       "      <td>Cancer Vaccine Has Been Approved For Human Tri...</td>\n",
       "      <td>https://www.facebook.com/ScienceNaturePage/vid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancer Vaccine Has Been Approved For Human Trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>373126.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Page Name                 User Name       Page Id  \\\n",
       "0                   Planet Paws             PlanetPaws.ca  1.124380e+14   \n",
       "1  Natalie Bomke Fox 32 Chicago  NatalieBomkeFox32Chicago  1.668040e+14   \n",
       "2                 FOX 2 Detroit           WJBKFox2Detroit  3.636580e+11   \n",
       "3                       Gizmodo                   gizmodo  5.718759e+09   \n",
       "4              Hashem Al-Ghaili         ScienceNaturePage  6.935050e+14   \n",
       "\n",
       "  Page Likes at Posting                  Created          Type   Likes  \\\n",
       "0               1578676  2017-02-26 09:50:24 EST  Native Video   75461   \n",
       "1                112316  2018-02-02 09:30:00 EST  Native Video  135485   \n",
       "2                819062  2018-02-05 14:45:02 EST  Native Video   84593   \n",
       "3               1555197  2017-11-20 17:40:46 EST  Native Video  122206   \n",
       "4              13426659  2018-06-22 11:08:29 EDT  Native Video   40464   \n",
       "\n",
       "   Comments  Shares   Love  ...  \\\n",
       "0     42879  894482   2406  ...   \n",
       "1     41642  627972  27306  ...   \n",
       "2     15447  580983  14496  ...   \n",
       "3     20150  433945  11158  ...   \n",
       "4      3112  322544   3762  ...   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.facebook.com/PlanetPaws.ca/posts/1...   \n",
       "1  https://www.facebook.com/NatalieBomkeFox32Chic...   \n",
       "2  https://www.facebook.com/WJBKFox2Detroit/posts...   \n",
       "3  https://www.facebook.com/gizmodo/posts/1015594...   \n",
       "4  https://www.facebook.com/ScienceNaturePage/pos...   \n",
       "\n",
       "                                             Message  \\\n",
       "0  Over-vaccinating and the overdosing of pet vac...   \n",
       "1  CANCER VACCINE SUCCESSFUL IN TESTING... Stanfo...   \n",
       "2  CANCER VACCINE SUCCESSFUL IN TESTING: The vacc...   \n",
       "3  Paul Alexander spends nearly every hour, of ev...   \n",
       "4  Cancer Vaccine Has Been Approved For Human Tri...   \n",
       "\n",
       "                                                Link  Final Link  \\\n",
       "0  https://www.facebook.com/PlanetPaws.ca/videos/...         NaN   \n",
       "1  https://www.facebook.com/NatalieBomkeFox32Chic...         NaN   \n",
       "2  https://www.facebook.com/WJBKFox2Detroit/video...         NaN   \n",
       "3  https://www.facebook.com/gizmodo/videos/101559...         NaN   \n",
       "4  https://www.facebook.com/ScienceNaturePage/vid...         NaN   \n",
       "\n",
       "                                           Link Text Description  Sponsor Id  \\\n",
       "0                  The Dangers of Vaccine Overdosing         NaN         NaN   \n",
       "1            CANCER VACCINE SUCCESSFUL IN TESTING...         NaN         NaN   \n",
       "2                          CANCER VACCINE SUCCESSFUL         NaN         NaN   \n",
       "3                         The Last of the Iron Lungs         NaN         NaN   \n",
       "4  Cancer Vaccine Has Been Approved For Human Trials         NaN         NaN   \n",
       "\n",
       "   Sponsor Name      Score Yes  \n",
       "0           NaN  1091367.0   1  \n",
       "1           NaN   861565.0   1  \n",
       "2           NaN   712774.0   1  \n",
       "3           NaN   674298.0   1  \n",
       "4           NaN   373126.0   1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('FBVaxData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identify the posts to be used to train the model and extract these posts from the dataset\n",
    "Identify the posts to be used for training and testing the model. \n",
    "<br>\n",
    "- Anti-vaccine posts are chosen from the seven Facebook pages that generate the top 20% of the anti-vaccine posts. There were 1429 posts.\n",
    "- In order to have close sample size for pro-vaccine posts, I choose posts from the as many Facebook page that are pro-vaccine as necessary. There are 1182 posts.\n",
    "<br>\n",
    "\n",
    "** See [Alexis's data](https://docs.google.com/spreadsheets/d/1j6tJDlMJErjBwoxLh4GpzCV4nM00nQS_1faZMXPGcxw/edit#gid=1593598275) (the 3rd spreadsheet, 'Copy of Pivot Table 1') and [her article](https://www.theatlantic.com/health/archive/2019/02/anti-vaxx-facebook-social-media/583681/) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anti-vaxx Posts:  1429\n",
      "Number of Pro-vaxx Posts:  1182\n"
     ]
    }
   ],
   "source": [
    "AntiVaxSource = ['NaturalNews.com', 'Dr. Tenpenny on Vaccines and Current Events', 'Stop Mandatory Vaccination', \n",
    "                 'March Against Monsanto', 'J. B. Handley', 'Erin at Health Nut News', 'Revolution For Choice']\n",
    "ProVaxSource = ['I fucking love science', 'SciBabe', 'The Credible Hulk', 'National Vaccine Information Center',\n",
    "                'Gavi, the Vaccine Alliance', 'Refutations to Anti-Vaccine Memes', 'Do you even Science, Bro',\n",
    "                'Stop the Anti-Science Movement', 'We Love GMOs and Vaccines', 'NPR', \n",
    "                'World Health Organization (WHO)', 'March for Science', 'ScienceAlert', \n",
    "                \"The Skeptics' Guide to the Universe\", 'Futurism', 'Being Liberal', 'A Science Enthusiast',\n",
    "                'ZDoggMD', 'Insufferably Intolerant Science Nerd', 'Now This']\n",
    "\n",
    "def CalPostNumber(SourceList):\n",
    "    Num = 0\n",
    "    for s in SourceList:\n",
    "        for p in data['Page Name']:\n",
    "            if p == s:\n",
    "                Num += 1\n",
    "    return Num\n",
    "\n",
    "print('Number of Anti-vaxx Posts: ', CalPostNumber(AntiVaxSource))\n",
    "print('Number of Pro-vaxx Posts: ', CalPostNumber(ProVaxSource))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label each post as 'ProVax' or 'AntiVax'\n",
    "Label the data with 'ProVax' and 'AntiVax'. Drop the posts with missing data as well as the data that is not needed for the project (i.e., the posts that don't belong to the selected sources).\n",
    "<br>\n",
    "At this point, I have a dataframe with three columns: index, message, and label. Again, display the first 5 rows for double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reesetou/anaconda3/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/reesetou/anaconda3/envs/python37/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go vaccines!</td>\n",
       "      <td>ProVax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's like the MMR jab, but for heroin.</td>\n",
       "      <td>ProVax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 90 mice infected with cancer, 87 of them we...</td>\n",
       "      <td>ProVax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Girls who received the HPV vaccine are much le...</td>\n",
       "      <td>ProVax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mainstream news reporting that #BigPharma is p...</td>\n",
       "      <td>AntiVax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message    Label\n",
       "0                                       Go vaccines!   ProVax\n",
       "1             It's like the MMR jab, but for heroin.   ProVax\n",
       "2  In 90 mice infected with cancer, 87 of them we...   ProVax\n",
       "3  Girls who received the HPV vaccine are much le...   ProVax\n",
       "4  Mainstream news reporting that #BigPharma is p...  AntiVax"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Page Name', 'Message']]\n",
    "data['Label'] = np.NaN\n",
    "\n",
    "def Labeling(SourceList, LabelName): # Only posts from the selected sources will be assigned a label\n",
    "    for s in SourceList:\n",
    "        for i in range(0, len(data)):\n",
    "            if data['Page Name'][i] == s:\n",
    "                data['Label'][i] = LabelName\n",
    "    return data\n",
    "\n",
    "data = Labeling(AntiVaxSource, 'AntiVax')\n",
    "data = Labeling(ProVaxSource, 'ProVax')\n",
    "data = data.dropna() # This will drop both posts with missing data & data without a label\n",
    "data = data.reset_index(drop = True) # Reset the index after dropping data points\n",
    "data = data.drop(columns = ['Page Name']) # No longer need the source for the following analyses\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the final sample size:\n",
    "- Anti-vaccine posts: 1355\n",
    "- Pro-vaccine posts: 1031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anti-vaxx Posts:  1355\n",
      "Number of Pro-vaxx Posts:  1031\n"
     ]
    }
   ],
   "source": [
    "print('Number of Anti-vaxx Posts: ', len(data[data['Label'] == 'AntiVax']))\n",
    "print('Number of Pro-vaxx Posts: ', len(data[data['Label'] == 'ProVax']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process the message using Natural Language Processing techniques\n",
    "Process each message by\n",
    "- Remove the hyperlink that was included in the message\n",
    "- Remove punctuation\n",
    "- Convert the words to lowercase and tokenize the words\n",
    "- Stemmerize the words\n",
    "- Filter out the stop words that were included in the message\n",
    "<br>\n",
    "\n",
    "At this point, each message is converted to a list of word stems in lowercase. All the hyperlinks, punctuations, emojis, and stop words are removed. Stop words are the words that don't bring semantic significance to the context, such as \"a,\" \"and,\" \"but.\"\n",
    "\n",
    "Convert the dataframe to a list that has the following information for each post\n",
    "- processed message\n",
    "- Label of the message\n",
    "<br>\n",
    "\n",
    "Display the first 2 data points for double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['antivaxxerlog'], 'ProVax'), (['cure', 'step', 'closer'], 'ProVax')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ProcessMessages():\n",
    "    for i in range(0, len(data)):\n",
    "        NoLink = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", data['Message'][i])\n",
    "        NoPunc = re.sub(r'[^\\w\\s]','',NoLink)\n",
    "        WordTokens = word_tokenize(NoPunc.lower())\n",
    "        ps = PorterStemmer()\n",
    "        WordStems = [ps.stem(w) for w in WordTokens]\n",
    "        StopWords = set(stopwords.words('english'))\n",
    "        FilteredSentence = [w for w in WordStems if not w in StopWords] \n",
    "        data['Message'][i] = FilteredSentence\n",
    "    return data\n",
    "\n",
    "data = ProcessMessages()\n",
    "\n",
    "documents = []\n",
    "\n",
    "def CreateDoc():\n",
    "    for i in range(0, len(data)):\n",
    "        documents.append((data['Message'][i], data['Label'][i]))\n",
    "    return data\n",
    "\n",
    "CreateDoc()\n",
    "random.shuffle(documents) # Randomize the order of the data points\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list that contains all the words that are used, and calculate the how often each word is used. Print out the 30 most commonly used words and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vaccin', 2474), ('thi', 774), ('children', 315), ('wa', 266), ('flu', 234), ('doctor', 214), ('get', 211), ('ha', 210), ('us', 206), ('inform', 202), ('year', 200), ('one', 198), ('caus', 193), ('antivaccin', 190), ('health', 184), ('know', 175), ('peopl', 172), ('measl', 170), ('like', 168), ('parent', 167), ('diseas', 160), ('hpv', 151), ('shot', 150), ('say', 138), ('death', 129), ('time', 128), ('hi', 128), ('whi', 126), ('free', 124), ('follow', 122)]\n"
     ]
    }
   ],
   "source": [
    "def CreateAllWords():\n",
    "    AllWords = []\n",
    "    for i in range(0, len(data)):\n",
    "        for w in data['Message'][i]:\n",
    "            AllWords.append(w)\n",
    "    return AllWords\n",
    "\n",
    "AllWords = CreateAllWords()\n",
    "\n",
    "AllWords = nltk.FreqDist(AllWords)\n",
    "print(AllWords.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data for training a machine learning model\n",
    "Use the 3000 most used words as features to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordFeatures = list(AllWords.keys())[:3000]\n",
    "\n",
    "def FindFeatures(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in WordFeatures:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post, screen through each word and mark whether the word is in the features (i.e., 3000 most used words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(FindFeatures(msg), label) for (msg, label) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSet = featuresets[:1900]\n",
    "TestingSet = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train models\n",
    "Train a Naive Bayes classifier model, report prediction accuracy percentage, and show the 15 most informative features.\n",
    "<br>\n",
    "\n",
    "The following results show that the Naive Bayes classifier model makes the correct prediction __82.71%__ of the time. Word like 'meme' is used 78 times more by pro-vaxxers than anti-vaxxers (probably to make fun of anti-vaxxers), and word like 'network' is used 38 times more by anti-vaxxers than pro-vaxxers.\n",
    "<br>\n",
    "\n",
    "** The words shown below as features are word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent:  82.71604938271605\n",
      "Most Informative Features\n",
      "                    meme = True           ProVax : AntiVa =     78.4 : 1.0\n",
      "                 network = True           AntiVa : ProVax =     38.1 : 1.0\n",
      "                industri = True           AntiVa : ProVax =     25.7 : 1.0\n",
      "                   truth = True           AntiVa : ProVax =     22.5 : 1.0\n",
      "                 resourc = True           AntiVa : ProVax =     21.3 : 1.0\n",
      "                  insert = True           AntiVa : ProVax =     20.7 : 1.0\n",
      "              antivaccin = True           ProVax : AntiVa =     20.7 : 1.0\n",
      "                  despit = True           ProVax : AntiVa =     15.9 : 1.0\n",
      "              mainstream = True           AntiVa : ProVax =     13.2 : 1.0\n",
      "                 coverag = True           ProVax : AntiVa =     12.4 : 1.0\n",
      "                antivaxx = True           ProVax : AntiVa =     12.3 : 1.0\n",
      "                    seri = True           AntiVa : ProVax =     11.7 : 1.0\n",
      "               pneumonia = True           ProVax : AntiVa =     11.6 : 1.0\n",
      "                   ebola = True           ProVax : AntiVa =     11.6 : 1.0\n",
      "               mandatori = True           AntiVa : ProVax =      9.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(TrainingSet)\n",
    "print(\"Original Naive Bayes Algo accuracy percent: \",(nltk.classify.accuracy(classifier, TestingSet))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train another seven classifiers.\n",
    "<br>\n",
    "\n",
    "First I try another two Naive Bayes classifiers. Naive Bayes classifier for multinomial models makes the correct prediction __82.72%__ of the time, and Naive Bayes classifier for multivariate Bernoulli models makes the correct prediction __83.33%__ of the time. I would think that Naive Bayes classifier for multivariate Bernoulli models should perform slightly better, because unlike Naive Bayes classifier for multinomial models, it is designed for binary/boolean features. My data is in binary format. However, the two models preform equally well here.\n",
    "\n",
    "Logistic regression classifier makes the correct prediction __81.48%__ of the time, and linear classifiers with stochastic gradient descent (SGD) training makes the correct prediction __81.28%__ of the time. Again, the two models perform equally well here.\n",
    "\n",
    "C-Support vector classification, linear support vector classification, and Nu-Support vector classification makes the correct prediction __58.85%__ , __80.04%__ , __78.60%__ of the time, respectively. C-Support vector classification performs the worst among these three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.8271604938271605\n",
      "BernoulliNB accuracy percent: 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reesetou/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Classifier accuracy percent: 81.48148148148148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reesetou/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Classifier accuracy percent: 81.27572016460906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reesetou/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Classifier accuracy percent: 58.8477366255144\n",
      "LinearSVC Classifier accuracy percent: 80.04115226337449\n",
      "NuSVC Classifier accuracy percent: 78.60082304526749\n"
     ]
    }
   ],
   "source": [
    "MNBClassifier = SklearnClassifier(MultinomialNB())\n",
    "MNBClassifier.train(TrainingSet)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNBClassifier, TestingSet))\n",
    "\n",
    "BNBClassifier = SklearnClassifier(BernoulliNB())\n",
    "BNBClassifier.train(TrainingSet)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNBClassifier, TestingSet))\n",
    "\n",
    "LogisticRegressionClassifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegressionClassifier.train(TrainingSet)\n",
    "print(\"LogisticRegression Classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegressionClassifier, TestingSet))*100)\n",
    "\n",
    "SGDClassifierClassifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifierClassifier.train(TrainingSet)\n",
    "print(\"SGDClassifier Classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifierClassifier, TestingSet))*100)\n",
    "\n",
    "SVCClassifier = SklearnClassifier(SVC())\n",
    "SVCClassifier.train(TrainingSet)\n",
    "print(\"SVC Classifier accuracy percent:\", (nltk.classify.accuracy(SVCClassifier, TestingSet))*100)\n",
    "\n",
    "LinearSVCClassifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVCClassifier.train(TrainingSet)\n",
    "print(\"LinearSVC Classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVCClassifier, TestingSet))*100)\n",
    "\n",
    "NuSVCClassifier = SklearnClassifier(NuSVC())\n",
    "NuSVCClassifier.train(TrainingSet)\n",
    "print(\"NuSVC Classifier accuracy percent:\", (nltk.classify.accuracy(NuSVCClassifier, TestingSet))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I ensemble several models that have similar performace to create a model that aggregate their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final model aggregates predictions from Naive Bayes classifier, Naive Bayes classifier for multinomial models, Naive Bayes classifier for multivariate Bernoulli models, logistic regression classifier, linear classifiers with stochastic gradient descent training, linear support vector classification, and Nu-Support vector classification. It makes the correct prediction __82.92%__ of the time. \n",
    "<br>\n",
    "\n",
    "I also use the model to predict the first 5 posts from the testing dataset. The confidence of the prediction is shown next to the predction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotedClassifier accuracy percent: 82.92181069958848\n",
      "Classification: ProVax Confidence %: 100.0\n",
      "Classification: AntiVax Confidence %: 100.0\n",
      "Classification: ProVax Confidence %: 100.0\n",
      "Classification: AntiVax Confidence %: 100.0\n",
      "Classification: ProVax Confidence %: 71.42857142857143\n",
      "Classification: ProVax Confidence %: 57.14285714285714\n"
     ]
    }
   ],
   "source": [
    "VotedClassifier = VoteClassifier(classifier,\n",
    "                                  NuSVCClassifier,\n",
    "                                  LinearSVCClassifier,\n",
    "                                  SGDClassifierClassifier,\n",
    "                                  MNBClassifier,\n",
    "                                  BNBClassifier,\n",
    "                                  LogisticRegressionClassifier)\n",
    "\n",
    "print(\"VotedClassifier accuracy percent:\", (nltk.classify.accuracy(VotedClassifier, TestingSet))*100)\n",
    "\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[0][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[0][0])*100)\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[1][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[1][0])*100)\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[2][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[2][0])*100)\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[3][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[3][0])*100)\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[4][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[4][0])*100)\n",
    "print(\"Classification:\", VotedClassifier.classify(TestingSet[5][0]), \"Confidence %:\",VotedClassifier.confidence(TestingSet[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "** The following are the lists and models that are used in the actual web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveWordFeatures = open(\"WordFeatures.pickle\",\"wb\")\n",
    "pickle.dump(WordFeatures, SaveWordFeatures)\n",
    "SaveWordFeatures.close()\n",
    "\n",
    "StopWords = set(stopwords.words('english'))\n",
    "\n",
    "SaveStopWords = open(\"StopWords.pickle\",\"wb\")\n",
    "pickle.dump(StopWords, SaveStopWords)\n",
    "SaveStopWords.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveClassifier = open(\"NaiveBayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, SaveClassifier)\n",
    "SaveClassifier.close()\n",
    "\n",
    "ClassifierF = open(\"NaiveBayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(ClassifierF)\n",
    "ClassifierF.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37]",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
